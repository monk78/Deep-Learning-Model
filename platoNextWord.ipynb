{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of The Republic, by Plato\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use i\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'pg1497.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'the', 'republic', 'author', 'plato', 'translator', 'b', 'jowett', 'posting', 'date', 'august', 'ebook', 'release', 'date', 'october', 'last', 'updated', 'june', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'produced', 'by', 'sue', 'asscher', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'the', 'republic', 'by', 'plato', 'jowett', 'etext', 'introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of', 'the']\n",
      "Total Tokens: 219633\n",
      "Unique Tokens: 10649\n"
     ]
    }
   ],
   "source": [
    "tokens = clean(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 219582\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'pluto.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'pluto.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            532500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10650)             1075650   \n",
      "=================================================================\n",
      "Total params: 1,759,050\n",
      "Trainable params: 1,759,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "219582/219582 [==============================] - 528s 2ms/step - loss: 6.1600 - acc: 0.0951\n",
      "Epoch 2/40\n",
      "219582/219582 [==============================] - 525s 2ms/step - loss: 5.8043 - acc: 0.1210\n",
      "Epoch 3/40\n",
      "219582/219582 [==============================] - 524s 2ms/step - loss: 5.6106 - acc: 0.1401\n",
      "Epoch 4/40\n",
      "219582/219582 [==============================] - 524s 2ms/step - loss: 5.4690 - acc: 0.1535\n",
      "Epoch 5/40\n",
      "219582/219582 [==============================] - 528s 2ms/step - loss: 5.3673 - acc: 0.1604\n",
      "Epoch 6/40\n",
      "219582/219582 [==============================] - 531s 2ms/step - loss: 5.2896 - acc: 0.1659\n",
      "Epoch 7/40\n",
      "219582/219582 [==============================] - 561s 3ms/step - loss: 5.2240 - acc: 0.1707\n",
      "Epoch 8/40\n",
      "219582/219582 [==============================] - 623s 3ms/step - loss: 5.2247 - acc: 0.1690\n",
      "Epoch 9/40\n",
      "219582/219582 [==============================] - 636s 3ms/step - loss: 5.1398 - acc: 0.1751\n",
      "Epoch 10/40\n",
      "219582/219582 [==============================] - 602s 3ms/step - loss: 5.0691 - acc: 0.1811\n",
      "Epoch 11/40\n",
      "219582/219582 [==============================] - 562s 3ms/step - loss: 5.0088 - acc: 0.1849\n",
      "Epoch 12/40\n",
      "219582/219582 [==============================] - 586s 3ms/step - loss: 4.9486 - acc: 0.1893\n",
      "Epoch 13/40\n",
      "219582/219582 [==============================] - 590s 3ms/step - loss: 4.8805 - acc: 0.1938\n",
      "Epoch 14/40\n",
      "219582/219582 [==============================] - 606s 3ms/step - loss: 4.8210 - acc: 0.1970\n",
      "Epoch 15/40\n",
      "219582/219582 [==============================] - 605s 3ms/step - loss: 4.7634 - acc: 0.1992\n",
      "Epoch 16/40\n",
      "219582/219582 [==============================] - 579s 3ms/step - loss: 4.7099 - acc: 0.2029\n",
      "Epoch 17/40\n",
      "219582/219582 [==============================] - 548s 2ms/step - loss: 4.6584 - acc: 0.2054\n",
      "Epoch 18/40\n",
      "219582/219582 [==============================] - 576s 3ms/step - loss: 4.6099 - acc: 0.2083\n",
      "Epoch 19/40\n",
      "219582/219582 [==============================] - 602s 3ms/step - loss: 4.5637 - acc: 0.2111\n",
      "Epoch 20/40\n",
      "219582/219582 [==============================] - 601s 3ms/step - loss: 4.5200 - acc: 0.2138\n",
      "Epoch 21/40\n",
      "219582/219582 [==============================] - 567s 3ms/step - loss: 4.4776 - acc: 0.2162\n",
      "Epoch 22/40\n",
      "219582/219582 [==============================] - 572s 3ms/step - loss: 4.4362 - acc: 0.2191\n",
      "Epoch 23/40\n",
      "219582/219582 [==============================] - 655s 3ms/step - loss: 4.3964 - acc: 0.2214\n",
      "Epoch 24/40\n",
      "219582/219582 [==============================] - 605s 3ms/step - loss: 4.3574 - acc: 0.2232\n",
      "Epoch 25/40\n",
      "219582/219582 [==============================] - 578s 3ms/step - loss: 4.3206 - acc: 0.2263\n",
      "Epoch 26/40\n",
      "219582/219582 [==============================] - 617s 3ms/step - loss: 4.2838 - acc: 0.2286\n",
      "Epoch 27/40\n",
      "219582/219582 [==============================] - 652s 3ms/step - loss: 4.2481 - acc: 0.2313\n",
      "Epoch 28/40\n",
      "219582/219582 [==============================] - 629s 3ms/step - loss: 4.2134 - acc: 0.2336\n",
      "Epoch 29/40\n",
      "219582/219582 [==============================] - 580s 3ms/step - loss: 4.1780 - acc: 0.2357\n",
      "Epoch 30/40\n",
      "219582/219582 [==============================] - 553s 3ms/step - loss: 4.1448 - acc: 0.2390\n",
      "Epoch 31/40\n",
      "219582/219582 [==============================] - 572s 3ms/step - loss: 4.1100 - acc: 0.2417\n",
      "Epoch 32/40\n",
      "219582/219582 [==============================] - 613s 3ms/step - loss: 4.0766 - acc: 0.2441\n",
      "Epoch 33/40\n",
      "219582/219582 [==============================] - 630s 3ms/step - loss: 4.0431 - acc: 0.2468\n",
      "Epoch 34/40\n",
      "219582/219582 [==============================] - 579s 3ms/step - loss: 4.0096 - acc: 0.2500\n",
      "Epoch 35/40\n",
      "219582/219582 [==============================] - 595s 3ms/step - loss: 3.9756 - acc: 0.2529\n",
      "Epoch 36/40\n",
      "219582/219582 [==============================] - 604s 3ms/step - loss: 3.9431 - acc: 0.2563\n",
      "Epoch 37/40\n",
      "219582/219582 [==============================] - 567s 3ms/step - loss: 3.9103 - acc: 0.2594\n",
      "Epoch 38/40\n",
      "121728/219582 [===============>..............] - ETA: 4:11 - loss: 3.8491 - acc: 0.2643"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=40)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
